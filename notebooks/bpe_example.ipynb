{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6427bd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List,Dict,Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7236623",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"low low low low low lower lower widest widest widest newest newest newest newest newest newest\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cf425dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(tokens: List[str], tokenized: bool = False):\n",
    "    histogram = {}\n",
    "    for token in tokens:\n",
    "        if tokenized:\n",
    "            token = token.encode()\n",
    "            token = tuple(token[i : i + 1] for i in range(len(token)))\n",
    "        if token not in histogram:\n",
    "            histogram[token] = 1\n",
    "        else:\n",
    "            histogram[token] += 1\n",
    "    return histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "867e5323",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.token_to_id: Dict[bytes, int] = {\"<|endoftext|>\".encode(): 0}\n",
    "        self.id_to_token: List[bytes] = [\"<|endoftext|>\".encode()]\n",
    "\n",
    "        for i in range(256):\n",
    "            byte = i.to_bytes(1)\n",
    "            self.token_to_id[byte] = len(self.id_to_token)\n",
    "            self.id_to_token.append(byte)\n",
    "\n",
    "    def add_token(self, token: bytes):\n",
    "        self.token_to_id[token] = len(self.id_to_token)\n",
    "        self.id_to_token.append(token)\n",
    "\n",
    "    def get_count(self, tokens: List[str]):\n",
    "        return get_count(tokens,True)\n",
    "\n",
    "    def fit(self, text: str, verbose: bool = False, merges: int = 1):\n",
    "        tokens = text.split(\" \")\n",
    "        token_histogram: Dict[Tuple[bytes], int] = self.get_count(tokens)\n",
    "        for i in range(merges):\n",
    "            pair_hist: Dict[Tuple[bytes], int] = {}\n",
    "            if verbose:\n",
    "                print(\"token hist: \", token_histogram)\n",
    "            for encoded_token, count in token_histogram.items():\n",
    "                for i in range(1, len(encoded_token)):\n",
    "                    pair = (encoded_token[i - 1], encoded_token[i])\n",
    "                    if pair not in pair_hist:\n",
    "                        pair_hist[pair] = count\n",
    "                    else:\n",
    "                        pair_hist[pair] += count\n",
    "\n",
    "            best = max(pair_hist, key=lambda x: (pair_hist.get(x), x))\n",
    "            # if verbose:\n",
    "            #    print(\"pair_list: \", pair_list)\n",
    "            tokenA = best[0]\n",
    "            tokenB = best[1]\n",
    "            new_token = tokenA + tokenB\n",
    "            self.add_token(new_token)\n",
    "            if verbose:\n",
    "                print(\"new_token: \", new_token)\n",
    "            # mergnount otken_histogram\n",
    "            # mus√≠m zrekonstruovat pair_hist\n",
    "            new_token_histogram: Dict[Tuple[bytes], int] = {}\n",
    "            for encoded_token, count in token_histogram.items():\n",
    "                new_encoded_token = []\n",
    "                i = 1\n",
    "                while i < len(encoded_token):\n",
    "                    combined = encoded_token[i - 1] + encoded_token[i]\n",
    "                    if combined == new_token:\n",
    "                        new_encoded_token.append(combined)\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_encoded_token.append(encoded_token[i - 1])\n",
    "                        i += 1\n",
    "                # print(i,len(encoded_token),encoded_token,new_encoded_token)\n",
    "                if (\n",
    "                    i == len(encoded_token)\n",
    "                    or not new_encoded_token\n",
    "                    or new_encoded_token[-1] != new_token\n",
    "                ):\n",
    "                    new_encoded_token.append(encoded_token[-1])\n",
    "                new_encoded_token = tuple(new_encoded_token)\n",
    "                new_token_histogram[new_encoded_token] = count\n",
    "            token_histogram = new_token_histogram\n",
    "        if verbose:\n",
    "            print(\"token hist: \", token_histogram)\n",
    "\n",
    "    def tranform(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75c4d467",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "055d379d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token hist:  {(b'l', b'o', b'w'): 5, (b'l', b'o', b'w', b'e', b'r'): 2, (b'w', b'i', b'd', b'e', b's', b't'): 3, (b'n', b'e', b'w', b'e', b's', b't'): 6}\n",
      "new_token:  b'st'\n",
      "token hist:  {(b'l', b'o', b'w'): 5, (b'l', b'o', b'w', b'e', b'r'): 2, (b'w', b'i', b'd', b'e', b'st'): 3, (b'n', b'e', b'w', b'e', b'st'): 6}\n",
      "new_token:  b'est'\n",
      "token hist:  {(b'l', b'o', b'w'): 5, (b'l', b'o', b'w', b'e', b'r'): 2, (b'w', b'i', b'd', b'est'): 3, (b'n', b'e', b'w', b'est'): 6}\n",
      "new_token:  b'ow'\n",
      "token hist:  {(b'l', b'ow'): 5, (b'l', b'ow', b'e', b'r'): 2, (b'w', b'i', b'd', b'est'): 3, (b'n', b'e', b'w', b'est'): 6}\n",
      "new_token:  b'low'\n",
      "token hist:  {(b'low',): 5, (b'low', b'e', b'r'): 2, (b'w', b'i', b'd', b'est'): 3, (b'n', b'e', b'w', b'est'): 6}\n",
      "new_token:  b'west'\n",
      "token hist:  {(b'low',): 5, (b'low', b'e', b'r'): 2, (b'w', b'i', b'd', b'est'): 3, (b'n', b'e', b'west'): 6}\n",
      "new_token:  b'ne'\n",
      "token hist:  {(b'low',): 5, (b'low', b'e', b'r'): 2, (b'w', b'i', b'd', b'est'): 3, (b'ne', b'west'): 6}\n"
     ]
    }
   ],
   "source": [
    "tokenizer.fit(text,verbose=True,merges=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8cd001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = text.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00cb36fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'low': 5, 'lower': 2, 'widest': 3, 'newest': 6}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_histogram = get_count(tokens)\n",
    "token_histogram"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
