{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "6427bd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List,Dict,Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "f7236623",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"low low low low low lower lower widest widest widest newest newest newest newest newest newest\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "0cf425dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(tokens: List[str], tokenized: bool = False):\n",
    "    histogram = {}\n",
    "    for token in tokens:\n",
    "        if tokenized:\n",
    "            token = token.encode()\n",
    "            token = tuple(token[i : i + 1] for i in range(len(token)))\n",
    "        if token not in histogram:\n",
    "            histogram[token] = 1\n",
    "        else:\n",
    "            histogram[token] += 1\n",
    "    return histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867e5323",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.token_to_id: Dict[bytes, int] = {\"<|endoftext|>\".encode(): 0}\n",
    "        self.id_to_token: List[bytes] = [\"<|endoftext|>\".encode()]\n",
    "\n",
    "        for i in range(256):\n",
    "            byte = i.to_bytes(1)\n",
    "            self.token_to_id[byte] = len(self.id_to_token)\n",
    "            self.id_to_token.append(byte)\n",
    "\n",
    "    def add_token(self, token: bytes):\n",
    "        self.token_to_id[token] = len(self.id_to_token)\n",
    "        self.id_to_token.append(token)\n",
    "\n",
    "    def fit(self, text: str, verbose: bool = False, merges: int = 1):\n",
    "        tokens = text.split(\" \")\n",
    "        token_histogram: Dict[Tuple[bytes], int] = get_count(tokens, tokenized=True)\n",
    "        for i in range(merges):\n",
    "            pair_hist: Dict[Tuple[bytes], int] = {}\n",
    "            if verbose:\n",
    "                print(\"token hist: \",token_histogram)\n",
    "            for encoded_token, count in token_histogram.items():\n",
    "                for i in range(1, len(encoded_token)):\n",
    "                    pair = (encoded_token[i - 1], encoded_token[i])\n",
    "                    if pair not in pair_hist:\n",
    "                        pair_hist[pair] = count\n",
    "                    else:\n",
    "                        pair_hist[pair] += count\n",
    "\n",
    "            pair_list = list(pair_hist.items())\n",
    "            pair_list.sort(key=lambda el: (el[1], el[0]), reverse=True)\n",
    "            #if verbose:\n",
    "            #    print(\"pair_list: \", pair_list)\n",
    "            tokenA = pair_list[0][0][0]\n",
    "            tokenB = pair_list[0][0][1]\n",
    "            new_token = tokenA + tokenB\n",
    "            self.add_token(new_token)\n",
    "            if verbose:\n",
    "                print(\"new_token: \", new_token)\n",
    "            # mergnount otken_histogram\n",
    "            # mus√≠m zrekonstruovat pair_hist\n",
    "            new_token_histogram: Dict[Tuple[bytes], int] = {}\n",
    "            for encoded_token, count in token_histogram.items():\n",
    "                new_encoded_token = []\n",
    "                i=1\n",
    "                while i < len(encoded_token):\n",
    "                    combined = encoded_token[i - 1] + encoded_token[i]\n",
    "                    if combined == new_token:\n",
    "                        new_encoded_token.append(combined)\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_encoded_token.append(encoded_token[i - 1])\n",
    "                        i += 1\n",
    "                #print(i,len(encoded_token),encoded_token,new_encoded_token)\n",
    "                if i == len(encoded_token) or not new_encoded_token or new_encoded_token[-1] != new_token:\n",
    "                    new_encoded_token.append(encoded_token[-1])\n",
    "                new_encoded_token = tuple(new_encoded_token)\n",
    "                new_token_histogram[new_encoded_token] = count\n",
    "            token_histogram = new_token_histogram\n",
    "        if verbose:\n",
    "            print(\"token hist: \",token_histogram)\n",
    "\n",
    "    def tranform(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "75c4d467",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "055d379d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token hist:  {(b'l', b'o', b'w'): 5, (b'l', b'o', b'w', b'e', b'r'): 2, (b'w', b'i', b'd', b'e', b's', b't'): 3, (b'n', b'e', b'w', b'e', b's', b't'): 6}\n",
      "new_token:  b'st'\n",
      "3 3 (b'l', b'o', b'w') [b'l', b'o']\n",
      "5 5 (b'l', b'o', b'w', b'e', b'r') [b'l', b'o', b'w', b'e']\n",
      "7 6 (b'w', b'i', b'd', b'e', b's', b't') [b'w', b'i', b'd', b'e', b'st']\n",
      "7 6 (b'n', b'e', b'w', b'e', b's', b't') [b'n', b'e', b'w', b'e', b'st']\n",
      "token hist:  {(b'l', b'o', b'w'): 5, (b'l', b'o', b'w', b'e', b'r'): 2, (b'w', b'i', b'd', b'e', b'st'): 3, (b'n', b'e', b'w', b'e', b'st'): 6}\n",
      "new_token:  b'est'\n",
      "3 3 (b'l', b'o', b'w') [b'l', b'o']\n",
      "5 5 (b'l', b'o', b'w', b'e', b'r') [b'l', b'o', b'w', b'e']\n",
      "6 5 (b'w', b'i', b'd', b'e', b'st') [b'w', b'i', b'd', b'est']\n",
      "6 5 (b'n', b'e', b'w', b'e', b'st') [b'n', b'e', b'w', b'est']\n",
      "token hist:  {(b'l', b'o', b'w'): 5, (b'l', b'o', b'w', b'e', b'r'): 2, (b'w', b'i', b'd', b'est'): 3, (b'n', b'e', b'w', b'est'): 6}\n",
      "new_token:  b'ow'\n",
      "4 3 (b'l', b'o', b'w') [b'l', b'ow']\n",
      "5 5 (b'l', b'o', b'w', b'e', b'r') [b'l', b'ow', b'e']\n",
      "4 4 (b'w', b'i', b'd', b'est') [b'w', b'i', b'd']\n",
      "4 4 (b'n', b'e', b'w', b'est') [b'n', b'e', b'w']\n",
      "token hist:  {(b'l', b'ow'): 5, (b'l', b'ow', b'e', b'r'): 2, (b'w', b'i', b'd', b'est'): 3, (b'n', b'e', b'w', b'est'): 6}\n",
      "new_token:  b'low'\n",
      "3 2 (b'l', b'ow') [b'low']\n",
      "4 4 (b'l', b'ow', b'e', b'r') [b'low', b'e']\n",
      "4 4 (b'w', b'i', b'd', b'est') [b'w', b'i', b'd']\n",
      "4 4 (b'n', b'e', b'w', b'est') [b'n', b'e', b'w']\n",
      "token hist:  {(b'low',): 5, (b'low', b'e', b'r'): 2, (b'w', b'i', b'd', b'est'): 3, (b'n', b'e', b'w', b'est'): 6}\n",
      "new_token:  b'west'\n",
      "1 1 (b'low',) []\n",
      "3 3 (b'low', b'e', b'r') [b'low', b'e']\n",
      "4 4 (b'w', b'i', b'd', b'est') [b'w', b'i', b'd']\n",
      "5 4 (b'n', b'e', b'w', b'est') [b'n', b'e', b'west']\n",
      "token hist:  {(b'low',): 5, (b'low', b'e', b'r'): 2, (b'w', b'i', b'd', b'est'): 3, (b'n', b'e', b'west'): 6}\n",
      "new_token:  b'ne'\n",
      "1 1 (b'low',) []\n",
      "3 3 (b'low', b'e', b'r') [b'low', b'e']\n",
      "4 4 (b'w', b'i', b'd', b'est') [b'w', b'i', b'd']\n",
      "3 3 (b'n', b'e', b'west') [b'ne']\n",
      "token hist:  {(b'low',): 5, (b'low', b'e', b'r'): 2, (b'w', b'i', b'd', b'est'): 3, (b'ne', b'west'): 6}\n"
     ]
    }
   ],
   "source": [
    "tokenizer.fit(text,verbose=True,merges=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "a8cd001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = text.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "00cb36fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'low': 5, 'lower': 2, 'widest': 3, 'newest': 6}"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_histogram = get_count(tokens)\n",
    "token_histogram"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
